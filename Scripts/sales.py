# -*- coding: utf-8 -*-
"""sales.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j46SX1sP8jJB_pxFDPsswWoOqLkpP2Ur
"""

def feature_selector(path: str, correlation_threshold: float, time_data: bool) -> list:
  """
    This function will select 30 features from given number of features.
    
    Overview of the parameters

    path: (str) - > path of the .csv file
    correlation_threshold : (float) - > Threshold value which will be used to remove multicollinearity
    time_data: (bool) - > Whether to consider datetime columns or not

    output:
    Funcation will return a list of selected 30 features
    """
  import pandas as pd
  from sklearn.preprocessing import OneHotEncoder
  import numpy as np
  import random

  #final columns
  final_columns = []

  #number of features
  no_features = 30

  #reading the dataframe
  data = pd.read_csv(path)
  print("CSV loaded succesfully")

  #dropping unwanted columns
  unwanted_columns = ['ID', 'Company_ID', 'Company_Name', 'Firstname', 'Surname', 'Address','Postcode', 'Phone']
  #dropping them
  data.drop(unwanted_columns, axis = 1, inplace = True)

  #checking duplicates
  duplicates = data.duplicated().sum()
  print("Number of duplicates: {}".format(duplicates))

  #if duplicates are present, drop them
  if duplicates > 0:
    data.drop_duplicates(inplace = True)
    data.reset_index(drop = True, inplace = True)

  #checking missing values
  if data.isnull().sum().any():
    print("Missing values identified")
    missing_values = data.isnull().sum()
    missing_values_col = list(missing_values.index)
    missing_cols = []
    #if missing values are present, logic to drop or fill them
    for num in range(len(missing_values)):
      if missing_values[num] > 0:
        #attaching the columns with missing values
        missing_cols.append(missing_values_col[num])

    data_con = data.select_dtypes(exclude = 'object')
    data_con_cols = list(data_con.columns)
    for cols in missing_cols:
      if cols in data_con_cols:
        print("Filling missing values with median for {}".format(cols))
        data[cols].fillna(data[cols].median(), inplace = True)
    
    #leftover missing values from objects
    #remove them
    print("Dropping missing values for categorical data")
    len_rows = data.shape[0]
    data.dropna(how = 'any', inplace = True)
    print("Rows affected: {}".format(len_rows - data.shape[0]))
    #resetting the index
    data.reset_index(drop = True, inplace = True)
  
  #classifying features to object,dateime and continuous
  #selecting object data
  data_object = data.select_dtypes(include = "object")

  #datetime features are also read as objects
  #seperating them through regex
  categorical_columns = []
  date_time_columns = []
  for columns in data_object.columns:
    if data_object[columns].str.contains("[a-zA-Z]", regex = True).all():
      categorical_columns.append(columns)
    else:
      date_time_columns.append(columns)
  print("Number of categorical columns: {}".format(len(categorical_columns)))
  print("Number of date_time_columns: {}".format(len(date_time_columns)))

  #handling categorical data
  #assume only one categorical column exist
  print("Handling Categorical data")
  data_cat = data[categorical_columns]
  data_cat_col = list(data_cat.columns)
  #only getting the continent value
  data_cat[data_cat_col[0]] = [string.split("/")[0] for string in data_cat[data_cat_col[0]].to_list()]

  #There are seven categories
  #reducing the dimension through number of records
  records = data_cat[data_cat_col[0]].value_counts()
  values = np.array(records.values)
  value_ration = values/values.sum()
  index_list = list(records.index)

  #getting indexes with ration less than 0.1
  low_cols = []
  for index,value in enumerate(value_ration):
    if value_ration[index] < 0.1:
      low_cols.append(index_list[index])
  #labeling them as others
  list_continent = []
  for element in data_cat[data_cat_col[0]].to_list():
    if element in low_cols:
      element = "Other"
      list_continent.append(element)
    else:
      list_continent.append(element)
  #attaching the new labels
  data_cat[data_cat_col[0]]  = list_continent

  #onehot encoding categorical variables
  ofc = OneHotEncoder(drop = 'first')
  ohc = ofc.fit(data_cat[data_cat.columns])
  array = ofc.fit_transform(data_cat[data_cat.columns])
  array = array.toarray()
  data_final_cat = pd.DataFrame(array, columns=ohc.get_feature_names_out())
  final_columns.extend(list(data_final_cat.columns))
  no_features -= len(data_final_cat.columns)
  print("Features selected: {}".format(list(data_final_cat.columns)))
  print("Features to find: {}".format(no_features))
  print("Handling categorical data is finished")

  #handling datetime data

  if time_data:
    print("Handling datetime data")
    data_time = data[date_time_columns]
    #since times are read as objects, converting them to datetime
    data_time = data_time.apply(pd.to_datetime)
    #getting the month
    #seasonal effect can be there, since this is running every 2 weeks
    for col in data_time.columns:
      data_time[col] = data_time[col].dt.month

    #validation if same values are present
    for cols in data_time.columns:
      if data_time[cols].nunique() == 1:
        data_time.drop(cols, axis = 1, inplace = True)
        print("{} is dropped".format(cols))
    #random feature will be selected from the number of columns
    #monthly seasonal effect will be measured by one random feature
    column_length = len(data_time.columns)
    data_time_columns = list(data_time.columns)
    print(data_time_columns)
    random_index = random.choice([i for i in range(1, column_length)])
    data_time_final = data_time[[data_time_columns[random_index]]]
    final_columns.extend(list(data_time_final.columns))
    no_features -= 1
    print("Features selected: {}".format(list(data_time_columns[random_index])))
    print("Features to find: {}".format(no_features))
    print("Datetime data handling is finished")

  #handling continuous data
  data_con = data.select_dtypes(exclude = "object")
  #removing multicollinearity if has
  data_con_corr = data_con.corr()
  columns = list(data_con.columns)
  correlated_columns = []

  for col_index in range(len(columns)):
    for row_index in range(len(columns)):
      if col_index == row_index:
        continue
      if abs(data_con_corr.iloc[row_index,col_index]) > correlation_threshold:
        correlated_columns.append(columns[col_index])
  if not correlated_columns:
    pass
  else:
    data_con.drop(correlated_columns, axis = 1, inplace = True)
  
  #sorting out the featuures which are having high variance for the modeling
  var = data_con.var()
  var_sorted = var.sort_values(ascending = False)

  #appending the highest variance columns to the final columns
  var_sorted_index = list(var_sorted.index)
  final_columns.extend(var_sorted_index[:no_features])
  print("Features selected: {}".format(var_sorted_index[:no_features]))
  print("Feature selection is completed")

  return final_columns
